{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import praw\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\"bot1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_data_for_range(start_date, end_date, subreddit):\n",
    "    query = f'https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&size=500&before={end_date}&after={start_date}'\n",
    "    post_data = requests.get(query)\n",
    "    return post_data.json()['data']\n",
    "    \n",
    "def filter_submission(total_submission_dict, post):\n",
    "    submission = reddit.submission(id=post['id'])\n",
    "    if (submission.link_flair_text is not None):\n",
    "        total_submission_dict['date'].append(post['created_utc'])\n",
    "        total_submission_dict['id'].append(post['id'])\n",
    "        total_submission_dict['flair'].append(submission.link_flair_text)\n",
    "        total_submission_dict['title'].append(post['title'])\n",
    "        total_submission_dict['selftext'].append(post['selftext'])\n",
    "\n",
    "def get_post_data(start_date, end_date, subreddit):\n",
    "    total_submission_dict = {\n",
    "        'date': [], \n",
    "        'id': [],\n",
    "        'flair': [],\n",
    "        'title': [],\n",
    "        'selftext': []\n",
    "    }\n",
    "    \n",
    "    post_data = get_post_data_for_range(start_date, end_date, subreddit)\n",
    "    while len(post_data) > 0:\n",
    "        for submission in post_data:\n",
    "            filter_submission(total_submission_dict, submission)\n",
    "\n",
    "        start_date = post_data[-1]['created_utc']\n",
    "        post_data = get_post_data_for_range(start_date, end_date, subreddit)\n",
    "    return total_submission_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geegees Post Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = ((datetime.datetime.utcnow() - relativedelta(years=1)))\n",
    "dates = [int((start_date + relativedelta(months=x)).timestamp()) for x in range(12)]\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    threads= [executor.submit(get_post_data, dates[month], dates[month+1], 'geegees') for month in range(0,len(dates),2)]\n",
    "    post_data_per_month = [thread.result() for thread in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>flair</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1576089234</td>\n",
       "      <td>e9b0ez</td>\n",
       "      <td>Image/Screenshot</td>\n",
       "      <td>campus blues, good luck on your finals üôåüèº</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1576487097</td>\n",
       "      <td>ebcs7g</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>A few more days everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1576507658</td>\n",
       "      <td>ebg18x</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Prof locked the door on an exam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1576559386</td>\n",
       "      <td>ebrb2u</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>That time of the year again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1580349688</td>\n",
       "      <td>evxpb0</td>\n",
       "      <td>Image/Screenshot</td>\n",
       "      <td>There, fixed it after the recent scientology a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      id             flair  \\\n",
       "0  1576089234  e9b0ez  Image/Screenshot   \n",
       "1  1576487097  ebcs7g          Shitpost   \n",
       "2  1576507658  ebg18x        Discussion   \n",
       "3  1576559386  ebrb2u          Shitpost   \n",
       "4  1580349688  evxpb0  Image/Screenshot   \n",
       "\n",
       "                                               title  \n",
       "0          campus blues, good luck on your finals üôåüèº  \n",
       "1                          A few more days everyone.  \n",
       "2                    Prof locked the door on an exam  \n",
       "3                        That time of the year again  \n",
       "4  There, fixed it after the recent scientology a...  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data was collected from cell above but at an earlier date\n",
    "df = pd.read_csv('geegees_data.csv')\n",
    "df.drop(['selftext'], axis=1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Aman\n",
      "[nltk_data]     Riat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>flair</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1576089234</td>\n",
       "      <td>e9b0ez</td>\n",
       "      <td>Image/Screenshot</td>\n",
       "      <td>campus blues, good luck on your finals üôåüèº</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1576487097</td>\n",
       "      <td>ebcs7g</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>A few more days everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1576507658</td>\n",
       "      <td>ebg18x</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Prof locked the door on an exam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1576559386</td>\n",
       "      <td>ebrb2u</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>That time of the year again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1580349688</td>\n",
       "      <td>evxpb0</td>\n",
       "      <td>Image/Screenshot</td>\n",
       "      <td>There, fixed it after the recent scientology a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      id             flair  \\\n",
       "0  1576089234  e9b0ez  Image/Screenshot   \n",
       "1  1576487097  ebcs7g          Shitpost   \n",
       "2  1576507658  ebg18x        Discussion   \n",
       "3  1576559386  ebrb2u          Shitpost   \n",
       "4  1580349688  evxpb0  Image/Screenshot   \n",
       "\n",
       "                                               title  \n",
       "0          campus blues, good luck on your finals üôåüèº  \n",
       "1                          A few more days everyone.  \n",
       "2                    Prof locked the door on an exam  \n",
       "3                        That time of the year again  \n",
       "4  There, fixed it after the recent scientology a...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import regex as re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "#Making sure titles are in english\n",
    "Word = list(set(words.words()))\n",
    "df = df[df['title'].str.contains('|'.join(Word))]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    353\n",
       "1     53\n",
       "Name: flair, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['flair'] = (df['flair'] == 'Shitpost').astype(int)\n",
    "df['flair'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample to remove bias towards non target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    93\n",
       "1    53\n",
       "Name: flair, dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_class_0, count_class_1 = df['flair'].value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df[df['flair'] == 0]\n",
    "df_class_1 = df[df['flair'] == 1]\n",
    "df_class_0 = df_class_0[0:93]\n",
    "df = pd.concat([df_class_0, df_class_1], axis=0)\n",
    "\n",
    "df['flair'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the titles\n",
    "def tokenize(x):\n",
    "    if not x:\n",
    "        x = ''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(x)\n",
    "df.loc[:,'tokens'] = df['title'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aman\n",
      "[nltk_data]     Riat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Lemming and stemming to reduce similar based words to a common word\n",
    "nltk.download('wordnet')\n",
    "def stemmer(x):\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in x])\n",
    " \n",
    "def lemmatize(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma'] = df['tokens'].map(lemmatize)\n",
    "df['stems'] = df['tokens'].map(stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['lemma']\n",
    "y = df['flair']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_pipe = Pipeline(steps = [('tf', TfidfVectorizer()), ('mnb', BernoulliNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    " 'tf__max_features' : [1000, 2000, 3000],\n",
    " 'tf__stop_words' : ['english', None],\n",
    " 'tf__ngram_range' : [(1,1),(1,2)],\n",
    " 'tf__use_idf' : [True, False],\n",
    " 'mnb__alpha' : [0.1, 0.5, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tf', TfidfVectorizer()),\n",
       "                                       ('mnb', BernoulliNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'mnb__alpha': [0.1, 0.5, 1],\n",
       "                         'tf__max_features': [1000, 2000, 3000],\n",
       "                         'tf__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tf__stop_words': ['english', None],\n",
       "                         'tf__use_idf': [True, False]})"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_mnb = GridSearchCV(bnb_pipe, param_grid,cv=5,n_jobs=-1)\n",
    "gscv_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not terrible but could be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027027027027027"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gscv_mnb.predict(X_test)\n",
    "gscv_mnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnb__alpha': 0.5,\n",
       " 'tf__max_features': 1000,\n",
       " 'tf__ngram_range': (1, 2),\n",
       " 'tf__stop_words': 'english',\n",
       " 'tf__use_idf': True}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_mnb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A lot of false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24,  0],\n",
       "       [11,  2]], dtype=int64)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be trying to use a transformer model in this section. Although I don't expect it to work well (due to the lack of data), why not try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('geegees_data.csv')\n",
    "df.drop(['selftext'], axis=1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['title'].str.contains('|'.join(Word))]\n",
    "df['flair'] = (df['flair'] == 'Shitpost').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_class_0, count_class_1 = df['flair'].value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df[df['flair'] == 0]\n",
    "df_class_1 = df[df['flair'] == 1]\n",
    "df_class_0 = df_class_0[0:93]\n",
    "df = pd.concat([df_class_0, df_class_1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_df= pd.DataFrame({\n",
    "    'text': df['title'].replace(r'\\n', ' ', regex=True),\n",
    "    'label':df['flair']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(transformer_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aman Riat\\Programming\\envs\\text_classification\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:386: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a92d983d3847708385dd2eaaeb67dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=116.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc32e11a6e64636a591e475ead33d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da39b8190f84d4f90e8700ab622fa8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 0 of 1'), FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aman Riat\\Programming\\envs\\text_classification\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aman Riat\\Programming\\envs\\text_classification\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:965: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171af8ceb8644335952450e7d4e401d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7f45d3e73a43f996adec0821ec8b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Evaluation'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aman Riat\\Programming\\envs\\text_classification\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "\n",
    "# Create a TransformerModel\n",
    "model = ClassificationModel('roberta', 'roberta-base', use_cuda=False, args={'overwrite_output_dir': True})\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train)\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test, acc=accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not great as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.0,\n",
       " 'tp': 0,\n",
       " 'tn': 17,\n",
       " 'fp': 0,\n",
       " 'fn': 13,\n",
       " 'acc': 0.5666666666666667,\n",
       " 'eval_loss': 0.6821510642766953}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gadgets Post Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collected for the last 15 days due to large amount of data that is usually posted within a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = ((datetime.datetime.utcnow() - relativedelta(days=15))).timestamp()\n",
    "end_date = datetime.datetime.utcnow().timestamp()\n",
    "last_15_days_data = get_post_data(1604881881, 1606100329, 'gadgets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(last_15_days_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VR / AR                 97\n",
       "Phones                  30\n",
       "Desktops / Laptops      23\n",
       "Computer peripherals    14\n",
       "Wearables               11\n",
       "Medical                 11\n",
       "Transportation          11\n",
       "Gaming                  10\n",
       "Home                    10\n",
       "Discussion               9\n",
       "Music                    7\n",
       "Watches                  6\n",
       "Rule 2                   5\n",
       "TV / Projectors          5\n",
       "Cameras                  4\n",
       "Misc                     4\n",
       "Homemade                 3\n",
       "Not A Gadget             2\n",
       "Phone Accessories        2\n",
       "Blogspam                 1\n",
       "*SMS-based               1\n",
       "Drones / UAVs            1\n",
       "Rule 5                   1\n",
       "Tablets                  1\n",
       "locked                   1\n",
       "Name: flair, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('gadgets_data.csv')\n",
    "df['flair'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VR / AR                 97\n",
       "Phones                  30\n",
       "Desktops / Laptops      23\n",
       "Computer peripherals    14\n",
       "Wearables               11\n",
       "Medical                 11\n",
       "Transportation          11\n",
       "Gaming                  10\n",
       "Home                    10\n",
       "Discussion               9\n",
       "Music                    7\n",
       "Watches                  6\n",
       "Rule 2                   5\n",
       "TV / Projectors          5\n",
       "Cameras                  4\n",
       "Misc                     4\n",
       "Homemade                 3\n",
       "Not A Gadget             2\n",
       "Phone Accessories        2\n",
       "Blogspam                 1\n",
       "*SMS-based               1\n",
       "Drones / UAVs            1\n",
       "Rule 5                   1\n",
       "Tablets                  1\n",
       "locked                   1\n",
       "Name: flair, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('gadgets_data.csv')\n",
    "df.drop(['selftext'], axis=1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df['flair'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>flair</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1604891928</td>\n",
       "      <td>jqpvp6</td>\n",
       "      <td>Not A Gadget</td>\n",
       "      <td>Maintenance Robot Walks on a Wind Turbine's Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1604896733</td>\n",
       "      <td>jqr3bg</td>\n",
       "      <td>Rule 2</td>\n",
       "      <td>Back here with, affordable smart watches in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1604897235</td>\n",
       "      <td>jqr7io</td>\n",
       "      <td>Music</td>\n",
       "      <td>iFi NEO iDSD: New HiRes Professional Headphone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1604899565</td>\n",
       "      <td>jqrr2a</td>\n",
       "      <td>Phones</td>\n",
       "      <td>moto G8 power lite launched in India | Specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1604900214</td>\n",
       "      <td>jqrwa4</td>\n",
       "      <td>Phones</td>\n",
       "      <td>moto G8 power lite launched in India | Specifi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      id         flair  \\\n",
       "0  1604891928  jqpvp6  Not A Gadget   \n",
       "1  1604896733  jqr3bg        Rule 2   \n",
       "2  1604897235  jqr7io         Music   \n",
       "3  1604899565  jqrr2a        Phones   \n",
       "4  1604900214  jqrwa4        Phones   \n",
       "\n",
       "                                               title  \n",
       "0  Maintenance Robot Walks on a Wind Turbine's Bl...  \n",
       "1  Back here with, affordable smart watches in th...  \n",
       "2  iFi NEO iDSD: New HiRes Professional Headphone...  \n",
       "3  moto G8 power lite launched in India | Specifi...  \n",
       "4  moto G8 power lite launched in India | Specifi...  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word = list(set(words.words()))\n",
    "df = df[df['title'].str.contains('|'.join(Word))]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets= list(df['flair'].value_counts()[0:5].index)\n",
    "target_df = df[df['flair'].isin(targets)]\n",
    "non_target_df = df[~df['flair'].isin(targets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Computer peripherals', 'Desktops / Laptops', 'Medical', 'Phones',\n",
       "       'VR / AR'], dtype=object)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flair with value 5 means other\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "\n",
    "le = LabelEncoder()\n",
    "target_df.loc[:,'flair'] = le.fit_transform(target_df['flair'])\n",
    "non_target_df.loc[:, 'flair'] = 5\n",
    "le.inverse_transform([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 91\n"
     ]
    }
   ],
   "source": [
    "#Comparing data split\n",
    "print(len(target_df), len(non_target_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([target_df, non_target_df], axis=0, join='outer', ignore_index=False, keys=None,\n",
    "          levels=None, names=None, verify_integrity=False, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemming and stemming to reduce similar based words to a common word\n",
    "df.loc[:,'tokens'] = target_df['title'].map(tokenize)\n",
    "df.dropna(inplace=True)\n",
    "df['lemma'] = df['tokens'].map(lemmatize)\n",
    "df['stems'] = df['tokens'].map(stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X = df['lemma']\n",
    "y = df['flair']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline(steps = [('tf', TfidfVectorizer()), ('mnb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    " 'tf__max_features' : [1000, 2000, 3000],\n",
    " 'tf__stop_words' : ['english', None],\n",
    " 'tf__ngram_range' : [(1,1),(1,2)],\n",
    " 'tf__use_idf' : [True, False],\n",
    " 'mnb__alpha' : [0.1, 0.5, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tf', TfidfVectorizer()),\n",
       "                                       ('mnb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'mnb__alpha': [0.1, 0.5, 1],\n",
       "                         'tf__max_features': [1000, 2000, 3000],\n",
       "                         'tf__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tf__stop_words': ['english', None],\n",
       "                         'tf__use_idf': [True, False]})"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_mnb = GridSearchCV(pipe_mnnb, param_grid,cv=5,n_jobs=-1)\n",
    "gscv_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gs_mnnb.predict(X_test)\n",
    "gscv_mnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnb__alpha': 0.1,\n",
       " 'tf__max_features': 1000,\n",
       " 'tf__ngram_range': (1, 1),\n",
       " 'tf__stop_words': 'english',\n",
       " 'tf__use_idf': False}"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  0,  0,  1],\n",
       "       [ 0,  3,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1,  2],\n",
       "       [ 0,  0,  0,  5,  2],\n",
       "       [ 0,  0,  0,  0, 28]], dtype=int64)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
